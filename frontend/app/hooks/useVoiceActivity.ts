"use client";

import { useState, useCallback, useEffect, useRef } from "react";
import { useMicVAD } from "@ricky0123/vad-react";
import { VAD_DEFAULT_OPTIONS, SPEECH_LOG_MAX_ENTRIES, WS_AUDIO_URL, AUDIO_SAMPLE_RATE } from "@/app/constants";
import { audioPlayer, AudioPlayerService } from "@/app/services";
import { audioSocket, ConnectionState, TranscriptMessage } from "@/app/services/audioSocket";
import { int16ToFloat32, arrayBufferToInt16, float32ToInt16, int16ToArrayBuffer } from "@/app/utils/audioEncoder";
import type { SpeechLogEntry } from "@/app/types";

interface UseVoiceActivityOptions {
  autoPlayback?: boolean;
  useEchoServer?: boolean;
}

export function useVoiceActivity(options: UseVoiceActivityOptions = {}) {
  const { autoPlayback = true, useEchoServer = true } = options;

  const [speechLog, setSpeechLog] = useState<SpeechLogEntry[]>([]);
  const [audioLevel, setAudioLevel] = useState(0);
  const [connectionState, setConnectionState] = useState<ConnectionState>("disconnected");
  const [subtitle, setSubtitle] = useState<string>("");
  const isConnectedRef = useRef(false);
  const subtitleTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê´€ë ¨
  const streamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const workletNodeRef = useRef<AudioWorkletNode | null>(null);
  const isSpeakingRef = useRef(false);
  const chunkCountRef = useRef(0);

  const addLogEntry = useCallback(
    (type: SpeechLogEntry["type"], message: string) => {
      const entry: SpeechLogEntry = {
        id: crypto.randomUUID(),
        timestamp: new Date().toLocaleTimeString(),
        type,
        message,
      };
      setSpeechLog((prev) =>
        [...prev, entry].slice(-SPEECH_LOG_MAX_ENTRIES)
      );
    },
    []
  );

  // WebSocket ì—°ê²° ê´€ë¦¬
  useEffect(() => {
    if (!useEchoServer) return;

    audioSocket.connect(WS_AUDIO_URL, {
      onConnectionChange: (state) => {
        setConnectionState(state);
        isConnectedRef.current = state === "connected";
        if (state === "connected") {
          addLogEntry("info", "ì„œë²„ ì—°ê²°ë¨");
        } else if (state === "disconnected") {
          addLogEntry("info", "ì„œë²„ ì—°ê²° ëŠê¹€");
        } else if (state === "error") {
          addLogEntry("error", "ì„œë²„ ì—°ê²° ì˜¤ë¥˜");
        }
      },
      onMessage: (data) => {
        if (data instanceof ArrayBuffer) {
          if (AudioPlayerService.isMp3(data)) {
            addLogEntry("end", "ğŸ”Š TTS ì¬ìƒ");
            audioPlayer.playMp3(data);
          } else {
            const int16Data = arrayBufferToInt16(data);
            const float32Data = int16ToFloat32(int16Data);
            audioPlayer.play(float32Data);
          }
        } else if (typeof data === "string") {
          try {
            const response = JSON.parse(data);
            if (response.status === "ready") {
              addLogEntry("info", `ì„¸ì…˜: ${response.session_id?.slice(0, 8)}...`);
            }
          } catch {
            console.log("Message:", data);
          }
        }
      },
      onTranscript: (transcript: TranscriptMessage) => {
        setSubtitle(transcript.text);
        addLogEntry("info", `ğŸ“ ${transcript.text}`);

        if (subtitleTimeoutRef.current) {
          clearTimeout(subtitleTimeoutRef.current);
        }
        subtitleTimeoutRef.current = setTimeout(() => {
          setSubtitle("");
        }, 5000);
      },
      onError: (error) => {
        console.error("WebSocket error:", error);
      },
    });

    return () => {
      audioSocket.disconnect();
    };
  }, [useEchoServer, addLogEntry]);

  // â˜… ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¬ë° ì„¤ì • (100ms ë‹¨ìœ„)
  useEffect(() => {
    console.log(`ğŸš€ Streaming useEffect: useEchoServer=${useEchoServer}`);
    if (!useEchoServer) return;

    let processorInterval: NodeJS.Timeout | null = null;
    let audioBuffer: Float32Array[] = [];

    const setupStreaming = async () => {
      console.log("ğŸš€ setupStreaming called");
      try {
        console.log("ğŸš€ Requesting getUserMedia...");
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,
            echoCancellation: true,
            noiseSuppression: true,
          }
        });
        streamRef.current = stream;

        // ê¸°ë³¸ ìƒ˜í”Œë ˆì´íŠ¸ ì‚¬ìš© (ë””ë°”ì´ìŠ¤ ë„¤ì´í‹°ë¸Œ)
        const audioContext = new AudioContext();
        audioContextRef.current = audioContext;
        const nativeSampleRate = audioContext.sampleRate;
        console.log(`ğŸ§ AudioContext created: state=${audioContext.state}, sampleRate=${nativeSampleRate}`);

        // AudioContextê°€ suspended ìƒíƒœë©´ resume
        if (audioContext.state === "suspended") {
          await audioContext.resume();
          console.log(`ğŸ§ AudioContext resumed: state=${audioContext.state}`);
        }

        const source = audioContext.createMediaStreamSource(stream);
        console.log(`ğŸ§ MediaStream connected: tracks=${stream.getAudioTracks().length}`);
        const analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        source.connect(analyser);

        // ScriptProcessorë¡œ ì˜¤ë””ì˜¤ ìº¡ì²˜ (deprecatedì§€ë§Œ ê°„ë‹¨í•¨)
        const processor = audioContext.createScriptProcessor(4096, 1, 1);
        source.connect(processor);
        processor.connect(audioContext.destination);
        console.log(`ğŸ§ ScriptProcessor connected: bufferSize=4096`);

        // ë¦¬ìƒ˜í”Œë§ í•¨ìˆ˜ (native -> 16kHz)
        const resampleTo16k = (input: Float32Array, fromRate: number): Float32Array => {
          const ratio = fromRate / AUDIO_SAMPLE_RATE;
          const outputLength = Math.floor(input.length / ratio);
          const output = new Float32Array(outputLength);
          for (let i = 0; i < outputLength; i++) {
            const srcIndex = i * ratio;
            const srcIndexFloor = Math.floor(srcIndex);
            const srcIndexCeil = Math.min(srcIndexFloor + 1, input.length - 1);
            const t = srcIndex - srcIndexFloor;
            output[i] = input[srcIndexFloor] * (1 - t) + input[srcIndexCeil] * t;
          }
          return output;
        };

        // 100ms @ 16kHz = 1600 samples, but at native rate it's different
        const CHUNK_SIZE_16K = 1600;
        const CHUNK_SIZE_NATIVE = Math.floor(CHUNK_SIZE_16K * (nativeSampleRate / AUDIO_SAMPLE_RATE));
        console.log(`ğŸ§ Chunk sizes: native=${CHUNK_SIZE_NATIVE} (${nativeSampleRate}Hz), target=1600 (16kHz)`);

        processor.onaudioprocess = (e) => {
          const inputData = e.inputBuffer.getChannelData(0);

          // ë””ë²„ê·¸: í”„ë¡œì„¸ì„œê°€ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸ (ê°€ë”ì”©ë§Œ)
          if (Math.random() < 0.02) {
            const maxAmp = Math.max(...Array.from(inputData).map(Math.abs));
            if (maxAmp > 0.01) {
              console.log(`ğŸ”Š Audio: amp=${maxAmp.toFixed(3)}, speaking=${isSpeakingRef.current}, connected=${isConnectedRef.current}`);
            }
          }

          if (!isSpeakingRef.current || !isConnectedRef.current) return;

          audioBuffer.push(new Float32Array(inputData));

          // ë²„í¼ê°€ 100ms ì´ìƒì´ë©´ ì „ì†¡ (ë„¤ì´í‹°ë¸Œ ìƒ˜í”Œë ˆì´íŠ¸ ê¸°ì¤€)
          const totalSamples = audioBuffer.reduce((acc, arr) => acc + arr.length, 0);
          if (totalSamples >= CHUNK_SIZE_NATIVE) {
            chunkCountRef.current++;
            const chunkNum = chunkCountRef.current;

            // ë„¤ì´í‹°ë¸Œ ìƒ˜í”Œë ˆì´íŠ¸ë¡œ í•©ì¹˜ê¸°
            const nativeChunk = new Float32Array(totalSamples);
            let offset = 0;
            for (const arr of audioBuffer) {
              nativeChunk.set(arr, offset);
              offset += arr.length;
            }

            // 16kHzë¡œ ë¦¬ìƒ˜í”Œë§
            const resampled = resampleTo16k(nativeChunk, nativeSampleRate);

            const durationMs = (resampled.length / AUDIO_SAMPLE_RATE) * 1000;
            const byteSize = resampled.length * 2;
            console.log(`ğŸ¤ [${chunkNum}] Sending: ${resampled.length} samples (${durationMs.toFixed(0)}ms, ${byteSize} bytes)`);
            addLogEntry("info", `ğŸ“¤ ì²­í¬ #${chunkNum}: ${durationMs.toFixed(0)}ms`);

            audioSocket.sendAudio(resampled);
            audioBuffer = [];
          }
        };

      } catch (err) {
        console.error("âŒ Audio setup failed:", err);
        addLogEntry("error", `ì˜¤ë””ì˜¤ ì„¤ì • ì‹¤íŒ¨: ${err}`);
      }
    };

    console.log("ğŸš€ Calling setupStreaming()...");
    setupStreaming();

    return () => {
      if (processorInterval) clearInterval(processorInterval);
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }
      if (audioContextRef.current) {
        audioContextRef.current.close();
      }
    };
  }, [useEchoServer, addLogEntry]);

  const vad = useMicVAD({
    ...VAD_DEFAULT_OPTIONS,
    onSpeechStart: () => {
      chunkCountRef.current = 0;
      addLogEntry("start", "ğŸ¤ ìŒì„± ê°ì§€ - ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘");
      console.log("ğŸ¤ Speech started - streaming begins");
      isSpeakingRef.current = true;
    },
    onSpeechEnd: () => {
      addLogEntry("end", `ìŒì„± ì¢…ë£Œ - ì´ ${chunkCountRef.current}ê°œ ì²­í¬ ì „ì†¡ë¨`);
      console.log(`ğŸ”‡ Speech ended - sent ${chunkCountRef.current} chunks`);
      isSpeakingRef.current = false;
    },
    onFrameProcessed: (probs) => {
      setAudioLevel(probs.isSpeech);
    },
  });

  const clearLog = useCallback(() => {
    setSpeechLog([]);
  }, []);

  useEffect(() => {
    return () => {
      if (subtitleTimeoutRef.current) {
        clearTimeout(subtitleTimeoutRef.current);
      }
    };
  }, []);

  return {
    ...vad,
    audioLevel,
    speechLog,
    clearLog,
    connectionState,
    subtitle,
  };
}
