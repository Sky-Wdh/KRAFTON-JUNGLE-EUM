import { AUDIO_SAMPLE_RATE } from "@/app/constants";
import { audioProcessor } from "./audioProcessor";
import { noiseSuppress } from "./noiseSuppress";

export interface AudioProcessingOptions {
  noiseSuppressionEnabled: boolean;
  dspEnabled: boolean;
}

export class AudioPlayerService {
  private audioContext: AudioContext | null = null;
  private options: AudioProcessingOptions = {
    noiseSuppressionEnabled: true,
    dspEnabled: true,
  };

  private getAudioContext(): AudioContext {
    if (!this.audioContext) {
      this.audioContext = new AudioContext({ sampleRate: AUDIO_SAMPLE_RATE });
    }
    return this.audioContext;
  }

  setOptions(options: Partial<AudioProcessingOptions>): void {
    this.options = { ...this.options, ...options };
  }

  getOptions(): AudioProcessingOptions {
    return { ...this.options };
  }

  /**
   * ì˜¤ë””ì˜¤ ì¬ìƒ íŒŒì´í”„ë¼ì¸:
   * 1. RNNoise (ë…¸ì´ì¦ˆ ì œê±°)
   * 2. DSP (í•˜ì´íŒ¨ìŠ¤ í•„í„° + ì»´í”„ë ˆì„œ + ê²Œì¸)
   * 3. ì¬ìƒ
   */
  async play(audioData: Float32Array): Promise<void> {
    const audioContext = this.getAudioContext();

    if (audioContext.state === "suspended") {
      await audioContext.resume();
    }

    let processedData = audioData;

    // 1. RNNoise ë…¸ì´ì¦ˆ ì œê±°
    if (this.options.noiseSuppressionEnabled) {
      try {
        processedData = await noiseSuppress.process(processedData);
      } catch (error) {
        console.warn("RNNoise processing failed, using original audio:", error);
      }
    }

    // 2. DSP ì²˜ë¦¬ (í•˜ì´íŒ¨ìŠ¤ í•„í„° + ì»´í”„ë ˆì„œ + ê²Œì¸)
    if (this.options.dspEnabled) {
      try {
        processedData = await audioProcessor.processAudio(processedData);
      } catch (error) {
        console.warn("DSP processing failed:", error);
      }
    }

    // 3. ì¬ìƒ
    const audioBuffer = audioContext.createBuffer(
      1,
      processedData.length,
      AUDIO_SAMPLE_RATE
    );
    const channelData = audioBuffer.getChannelData(0);
    channelData.set(processedData);

    const source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(audioContext.destination);
    source.start();
  }

  /**
   * MP3 ì˜¤ë””ì˜¤ ì¬ìƒ (TTS ì‘ë‹µìš©)
   * Web Audio APIì˜ decodeAudioDataë¥¼ ì‚¬ìš©í•˜ì—¬ MP3 ë””ì½”ë”©
   */
  async playMp3(mp3Data: ArrayBuffer): Promise<void> {
    const audioContext = this.getAudioContext();

    if (audioContext.state === "suspended") {
      await audioContext.resume();
    }

    try {
      // MP3 ë””ì½”ë”©
      const audioBuffer = await audioContext.decodeAudioData(mp3Data);

      // ì¬ìƒ
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);
      source.start();

      console.log(`ğŸ”Š Playing MP3: ${audioBuffer.duration.toFixed(2)}s`);
    } catch (error) {
      console.error("Failed to decode MP3 audio:", error);
    }
  }

  /**
   * MP3 ë°ì´í„°ì¸ì§€ í™•ì¸ (magic bytes ì²´í¬)
   */
  static isMp3(data: ArrayBuffer): boolean {
    if (data.byteLength < 3) return false;
    const bytes = new Uint8Array(data);
    // ID3 header (ID3v2 tag)
    if (bytes[0] === 0x49 && bytes[1] === 0x44 && bytes[2] === 0x33) {
      return true;
    }
    // MPEG Audio Frame sync (0xFF 0xFB, 0xFF 0xFA, 0xFF 0xF3, 0xFF 0xF2, 0xFF 0xE3, 0xFF 0xE2)
    if (bytes[0] === 0xFF && (bytes[1] & 0xE0) === 0xE0) {
      return true;
    }
    return false;
  }

  async close(): Promise<void> {
    noiseSuppress.destroy();
    await audioProcessor.close();
    if (this.audioContext) {
      await this.audioContext.close();
      this.audioContext = null;
    }
  }
}

export const audioPlayer = new AudioPlayerService();
